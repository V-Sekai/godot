
$assert BATCH_TILE % 32 == 0
$assert BATCH_TILE >= 32
$SIMD_TILE = BATCH_TILE // 32
$assert OP in ["ADD", "DIV", "RDIV", "MAX", "MIN", "MUL", "SUB", "RSUB", "SQRDIFF", "PRELU", "RPRELU"]
#include <assert.h>

#include "src/xnnpack/math.h"
#include "src/xnnpack/simd/f32-hvx.h"
#include "src/xnnpack/vbinary.h"

$_HEXAGON_OP_HVX = {
$  "ADD": lambda x: "xnn_add_f32(%s, vb)" % x,
$  "DIV": lambda x: "xnn_div_f32(%s, vb)" % x,
$  "RDIV": lambda x: "xnn_div_f32(vb, %s)" % x,
$  "MAX": lambda x: "xnn_max_f32(%s, vb)" % x,
$  "MIN": lambda x: "xnn_min_f32(%s, vb)" % x,
$  "MUL": lambda x: "xnn_mul_f32(%s, vb)" % x,
$  "SUB": lambda x: "xnn_sub_f32(%s, vb)" % x,
$  "RSUB": lambda x: "xnn_sub_f32(vb, %s)" % x,
$  "SQRDIFF": lambda x: "xnn_sub_f32(%s, vb)" % x,
$  "PRELU": lambda x: "xnn_mul_f32(%s, vb)" % x,
$  "RPRELU": lambda x: "xnn_mul_f32(%s, vb)" % x,
$}[OP]
void xnn_f32_v${OP.lower()}c_ukernel__hvx_u${BATCH_TILE}(
    size_t batch,
    const float* input_a,
    const float* input_b,
    float* output,
    const struct xnn_f32_default_params* restrict params)
{
  assert(batch != 0);
  assert(batch % sizeof(float) == 0);
  assert(input_a != NULL);
  assert(input_b != NULL);
  assert(output != NULL);

  HVX_Vector vb = xnn_set1_f32(*input_b);
  $if OP == "PRELU":
    const HVX_Vector vzero = xnn_zero_f32();
  $if BATCH_TILE > 32:
    for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
      $for N in range(SIMD_TILE):
        HVX_Vector va${N} = xnn_loadu_f32(input_a + ${N * 32});
      input_a += ${BATCH_TILE};

      $for N in range(SIMD_TILE):
        HVX_Vector vacc${N} = ${_HEXAGON_OP_HVX("va" + str(N))};

      $if OP == "SQRDIFF":
        $for N in range(SIMD_TILE):
          vacc${N} = xnn_mul_f32(vacc${N}, vacc${N});
      $elif OP == "PRELU":
        $for N in range(SIMD_TILE):
          const HVX_VectorPred vm${N} = Q6_Q_vcmp_gt_VsfVsf(va${N}, vzero);

        $for N in range(SIMD_TILE):
          vacc${N} = Q6_V_vmux_QVV(vm${N}, va${N}, vacc${N});

      $for N in range(SIMD_TILE):
        xnn_storeu_f32(output + ${N * 32}, vacc${N});
      output += ${BATCH_TILE};
    }
  for (; batch >= 32 * sizeof(float); batch -= 32 * sizeof(float)) {
    HVX_Vector va = xnn_loadu_f32(input_a);
    input_a += 32;

    HVX_Vector vacc = ${_HEXAGON_OP_HVX("va")};
    $if OP == "SQRDIFF":
      vacc = xnn_mul_f32(vacc, vacc);
    $elif OP == "PRELU":
      const HVX_VectorPred vm = Q6_Q_vcmp_gt_VsfVsf(va, vzero);
      vacc = Q6_V_vmux_QVV(vm, va, vacc);

    xnn_storeu_f32(output, vacc);
    output+= 32;
  }
  if XNN_UNLIKELY(batch != 0) {
    HVX_Vector va = xnn_load_tail_f32(input_a, batch >> XNN_LOG2_SIZEOF_FLOAT);

    HVX_Vector vacc = ${_HEXAGON_OP_HVX("va")};
    $if OP == "SQRDIFF":
      vacc = xnn_mul_f32(vacc, vacc);
    $elif OP == "PRELU":
      const HVX_VectorPred vm = Q6_Q_vcmp_gt_VsfVsf(va, vzero);
      vacc = Q6_V_vmux_QVV(vm, va, vacc);

    Q6_V_vstu_variable(output, batch, vacc);
  }
}
